{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from CountryNet import * \n",
    "from bs4 import BeautifulSoup\n",
    "from random import *\n",
    "import time\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"/Users/carterraha/Desktop/artists/lyricsArray.pickle\", \"rb\") as output_file:\n",
    "    loadtest = cPickle.load(output_file)\n",
    "with open(\"/Users/carterraha/Desktop/artists/lyricsArray2.pickle\", \"rb\") as output_file:\n",
    "    loadtest2 = cPickle.load(output_file)\n",
    "with open(\"/Users/carterraha/Desktop/artists/lyricsArray3.pickle\", \"rb\") as output_file:\n",
    "    loadtest3 = cPickle.load(output_file)\n",
    "    \n",
    "lyricsToProcess = loadtest + loadtest2 + loadtest3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processedLyrics = []\n",
    "for idx in range(len(lyricsToProcess)):\n",
    "    processedLyrics.append([lyricsToProcess[idx].find(\"font\").findAll(text=True)[0],lyricsToProcess[idx].find(\"h1\").findAll(text=True)[0].strip().split(\" lyrics -\")[0],str(lyricsToProcess[idx]).split(\"<br/><br/>\\n\")[1].strip().split(\"<br/>\\n            <br/>\\n\")[0].replace(\"<br/>\",'')])\n",
    "for idx, lyric in enumerate(processedLyrics):\n",
    "    lyric[2] = lyric[2].split(\"[Thanks\")[0]\n",
    "    lyric[2] = lyric[2].split(\"\"\"<script type=\"\"\")[0]\n",
    "    lyric[2] = lyric[2].replace(\"\\n\\nChorus:\", '')\n",
    "    lyric[2] = lyric[2].replace(\"\\n\\nChorus x2\", '')\n",
    "    lyric[2] = lyric[2].replace(\"\\n\\nChorus x3\", '')\n",
    "    lyric[2] = lyric[2].replace(\"\\n\\nChorus\", '')\n",
    "    lyric[2] = lyric[2].replace(\"\\n\\nCHORUS\", '')\n",
    "    lyric[2] = lyric[2].replace(\"\\n\\nREPEAT CHORUS\", '')\n",
    "    lyric[2] = lyric[2].replace(\"\\n\\n(Chorus)\", '')\n",
    "    processedLyrics[idx] = lyric\n",
    "    \n",
    "#Run twice oddly\n",
    "for index,lyric in enumerate(processedLyrics):\n",
    "    if \"NMPA\" in lyric[2]:\n",
    "        del processedLyrics[index]\n",
    "for index,lyric in enumerate(processedLyrics):\n",
    "    if lyric[1] in [\"Nuages\", \"Kim\",\"The Ballad of Nobeard\", \"Vous et Moi\",\"Vous Et Moi\", \"Bon Voyage\", \"Huckleberry Jam\",\"Down Yonder\",\"Turf's Up\",\"Departure\", \"American Offline\"]:\n",
    "        del processedLyrics[index]\n",
    "\n",
    "#Run twice oddly\n",
    "for index,lyric in enumerate(processedLyrics):\n",
    "    if \"NMPA\" in lyric[2]:\n",
    "        del processedLyrics[index]\n",
    "for index,lyric in enumerate(processedLyrics):\n",
    "    if lyric[1] in [\"Nuages\", \"Kim\",\"The Ballad of Nobeard\", \"Vous et Moi\",\"Vous Et Moi\", \"Bon Voyage\", \"Huckleberry Jam\",\"Down Yonder\",\"Turf's Up\",\"Departure\", \"American Offline\"]:\n",
    "        del processedLyrics[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'George Strait',\n",
       " u'Unwound',\n",
       " \"Give me a bottle,\\nOf your very best,\\nCause I've got a problem\\nI'm gonna drink off my chest.\\nI'm gonna spend the night,\\nGettin' down,\\nCause that woman that I had\\nWrapped around my finger just a come nwound.\\n\\nThat woman that I had wrapped around my\\nFinger just a come unwound,\\nShe kicked my out of the house and \\nTonight I'm whiskey bound.\\nWell I'm gonna be,\\nThe drunkest fool in town,\\nCause that woman that i had\\nWrapped around my finger just a come unwound.\\n\\nWell she packed my bags,\\nAnd opened up the door,\\nAnd I got a feelin she didnt want me around no more.\\nShe caught me in a lie,\\nWhen I was messin around,\\nAnd that woman that i had \\nWrapped around my finger just a come unwound.\\n                    \\nThat woman that I had wrapped around my\\nFinger just a come unwound,\\nShe kicked my out of the house and \\nTonight I'm whiskey bound.\\nWell I'm gonna be,\\nThe drunkest fool in town,\\nCause that woman that i had\\nWrapped around my finger just a come unwound.\\nJust a come unwound\\n\\nAnd that woman that I had \\nWrapped around my finger just a come unwound.\\n\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData = []\n",
    "for lyric in processedLyrics[:350]:\n",
    "    trainingData.append(lyric[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tokenizerPM\n",
    "from utils import load_data, load_model_parameters_theano, generate_sentences\n",
    "from gru_theano import *\n",
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import operator\n",
    "from gru_theano import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testLyric = processedLyrics[1][2]\n",
    "wholeSong = \"SENTENCE_START \" + testLyric.replace(\"\\n\", \" \").strip() + \" SENTENCE_END\"\n",
    "x_train = wholeSong[:-1]\n",
    "y_train = wholeSong[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRUTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=128, bptt_truncate=-1):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Initialize the network parameters\n",
    "        E = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        U = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (6, hidden_dim, hidden_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (6, hidden_dim, hidden_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        b = np.zeros((6, hidden_dim))\n",
    "        c = np.zeros(word_dim)\n",
    "        # Theano: Created shared variables\n",
    "        self.E = theano.shared(name='E', value=E.astype(theano.config.floatX))\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        self.b = theano.shared(name='b', value=b.astype(theano.config.floatX))\n",
    "        self.c = theano.shared(name='c', value=c.astype(theano.config.floatX))\n",
    "        # SGD / rmsprop: Initialize parameters\n",
    "        self.mE = theano.shared(name='mE', value=np.zeros(E.shape).astype(theano.config.floatX))\n",
    "        self.mU = theano.shared(name='mU', value=np.zeros(U.shape).astype(theano.config.floatX))\n",
    "        self.mV = theano.shared(name='mV', value=np.zeros(V.shape).astype(theano.config.floatX))\n",
    "        self.mW = theano.shared(name='mW', value=np.zeros(W.shape).astype(theano.config.floatX))\n",
    "        self.mb = theano.shared(name='mb', value=np.zeros(b.shape).astype(theano.config.floatX))\n",
    "        self.mc = theano.shared(name='mc', value=np.zeros(c.shape).astype(theano.config.floatX))\n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        E, V, U, W, b, c = self.E, self.V, self.U, self.W, self.b, self.c\n",
    "        \n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        \n",
    "        def forward_prop_step(x_t, s_t1_prev, s_t2_prev):\n",
    "            # This is how we calculated the hidden state in a simple RNN. No longer!\n",
    "            # s_t = T.tanh(U[:,x_t] + W.dot(s_t1_prev))\n",
    "            \n",
    "            # Word embedding layer\n",
    "            x_e = E[:,x_t]\n",
    "            \n",
    "            # GRU Layer 1\n",
    "            z_t1 = T.nnet.hard_sigmoid(U[0].dot(x_e) + W[0].dot(s_t1_prev) + b[0])\n",
    "            r_t1 = T.nnet.hard_sigmoid(U[1].dot(x_e) + W[1].dot(s_t1_prev) + b[1])\n",
    "            c_t1 = T.tanh(U[2].dot(x_e) + W[2].dot(s_t1_prev * r_t1) + b[2])\n",
    "            s_t1 = (T.ones_like(z_t1) - z_t1) * c_t1 + z_t1 * s_t1_prev\n",
    "            \n",
    "            # GRU Layer 2\n",
    "            z_t2 = T.nnet.hard_sigmoid(U[3].dot(s_t1) + W[3].dot(s_t2_prev) + b[3])\n",
    "            r_t2 = T.nnet.hard_sigmoid(U[4].dot(s_t1) + W[4].dot(s_t2_prev) + b[4])\n",
    "            c_t2 = T.tanh(U[5].dot(s_t1) + W[5].dot(s_t2_prev * r_t2) + b[5])\n",
    "            s_t2 = (T.ones_like(z_t2) - z_t2) * c_t2 + z_t2 * s_t2_prev\n",
    "            \n",
    "            # Final output calculation\n",
    "            # Theano's softmax returns a matrix with one row, we only need the row\n",
    "            o_t = T.nnet.softmax(V.dot(s_t2) + c)[0]\n",
    "\n",
    "            return [o_t, s_t1, s_t2]\n",
    "        \n",
    "        [o, s, s2], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=x,\n",
    "            truncate_gradient=self.bptt_truncate,\n",
    "            outputs_info=[None, \n",
    "                          dict(initial=T.zeros(self.hidden_dim)),\n",
    "                          dict(initial=T.zeros(self.hidden_dim))])\n",
    "        \n",
    "        prediction = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        # Total cost (could add regularization here)\n",
    "        cost = o_error\n",
    "        \n",
    "        # Gradients\n",
    "        dE = T.grad(cost, E)\n",
    "        dU = T.grad(cost, U)\n",
    "        dW = T.grad(cost, W)\n",
    "        db = T.grad(cost, b)\n",
    "        dV = T.grad(cost, V)\n",
    "        dc = T.grad(cost, c)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.predict = theano.function([x], o)\n",
    "        self.predict_class = theano.function([x], prediction)\n",
    "        self.ce_error = theano.function([x, y], cost)\n",
    "        self.bptt = theano.function([x, y], [dE, dU, dW, db, dV, dc])\n",
    "        \n",
    "        # SGD parameters\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        decay = T.scalar('decay')\n",
    "        \n",
    "        # rmsprop cache updates\n",
    "        mE = decay * self.mE + (1 - decay) * dE ** 2\n",
    "        mU = decay * self.mU + (1 - decay) * dU ** 2\n",
    "        mW = decay * self.mW + (1 - decay) * dW ** 2\n",
    "        mV = decay * self.mV + (1 - decay) * dV ** 2\n",
    "        mb = decay * self.mb + (1 - decay) * db ** 2\n",
    "        mc = decay * self.mc + (1 - decay) * dc ** 2\n",
    "        \n",
    "        self.sgd_step = theano.function(\n",
    "            [x, y, learning_rate, theano.Param(decay, default=0.9)],\n",
    "            [], \n",
    "            updates=[(E, E - learning_rate * dE / T.sqrt(mE + 1e-6)),\n",
    "                     (U, U - learning_rate * dU / T.sqrt(mU + 1e-6)),\n",
    "                     (W, W - learning_rate * dW / T.sqrt(mW + 1e-6)),\n",
    "                     (V, V - learning_rate * dV / T.sqrt(mV + 1e-6)),\n",
    "                     (b, b - learning_rate * db / T.sqrt(mb + 1e-6)),\n",
    "                     (c, c - learning_rate * dc / T.sqrt(mc + 1e-6)),\n",
    "                     (self.mE, mE),\n",
    "                     (self.mU, mU),\n",
    "                     (self.mW, mW),\n",
    "                     (self.mV, mV),\n",
    "                     (self.mb, mb),\n",
    "                     (self.mc, mc)\n",
    "                    ], allow_input_downcast=True)\n",
    "        \n",
    "        \n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y) for y in Y])\n",
    "        return self.calculate_total_loss(X,Y)/float(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.001, nepoch=20, decay=0.9,\n",
    "    callback_every=10000, callback=None):\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # For each training example...\n",
    "        for i in np.random.permutation(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[0], y_train[0], learning_rate, decay)\n",
    "            num_examples_seen += 1\n",
    "            # Optionally do callback\n",
    "            if (callback and callback_every and num_examples_seen % callback_every == 0):\n",
    "                callback(model, num_examples_seen)            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carterraha/Code/FT_GRU/FT_GRU/lib/python2.7/site-packages/ipykernel_launcher.py:105: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n"
     ]
    }
   ],
   "source": [
    "gru_country = GRUTheano(len(index_to_word), 128, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_START_TOKEN = \"SENTENCE_START\"\n",
    "SENTENCE_END_TOKEN = \"SENTENCE_END\"\n",
    "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
    "wholeSong = [nltk.word_tokenize(wholeSong)]\n",
    "word_freq = nltk.FreqDist(itertools.chain(*wholeSong))\n",
    "vocab = sorted(word_freq.items(), key=lambda x: (x[1], x[0]), reverse=True)[:len(np.unique(wholeSong))]\n",
    "sorted_vocab = sorted(vocab, key=operator.itemgetter(1))\n",
    "index_to_word = [\"<MASK/>\", UNKNOWN_TOKEN] + [x[0] for x in sorted_vocab]\n",
    "word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(wholeSong):\n",
    "    wholeSong[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]\n",
    "\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in wholeSong])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in wholeSong])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru_country2 = train_with_sgd(gru_country, X_train, y_train, .001, 10000, .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SENTENCE_START',\n",
       "  'July',\n",
       "  'moonlight',\n",
       "  'shines',\n",
       "  'Your',\n",
       "  'pretty',\n",
       "  'little',\n",
       "  'head',\n",
       "  'on',\n",
       "  'my',\n",
       "  'shoulder',\n",
       "  'Pull',\n",
       "  'over',\n",
       "  'on',\n",
       "  'the',\n",
       "  'side',\n",
       "  'of',\n",
       "  'the',\n",
       "  'road',\n",
       "  'Oh',\n",
       "  'my',\n",
       "  'God',\n",
       "  ',',\n",
       "  'you',\n",
       "  \"'re\",\n",
       "  'something',\n",
       "  'Like',\n",
       "  'nothing',\n",
       "  'I',\n",
       "  \"'ve\",\n",
       "  'ever',\n",
       "  'seen',\n",
       "  'If',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'asleep',\n",
       "  'girl',\n",
       "  ',',\n",
       "  'let',\n",
       "  'me',\n",
       "  'dream',\n",
       "  'Baby',\n",
       "  'fall',\n",
       "  'into',\n",
       "  'my',\n",
       "  'kiss',\n",
       "  'It',\n",
       "  'should',\n",
       "  'just',\n",
       "  'happen',\n",
       "  'like',\n",
       "  'this',\n",
       "  'Trust',\n",
       "  'it',\n",
       "  'so',\n",
       "  'much',\n",
       "  'that',\n",
       "  'there',\n",
       "  \"'s\",\n",
       "  'no',\n",
       "  'one',\n",
       "  'else',\n",
       "  'but',\n",
       "  'us',\n",
       "  'and',\n",
       "  'This',\n",
       "  'moment',\n",
       "  'that',\n",
       "  'says',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'so',\n",
       "  'right',\n",
       "  \"'Cause\",\n",
       "  'that',\n",
       "  \"'s\",\n",
       "  'all',\n",
       "  'we',\n",
       "  'have',\n",
       "  'in',\n",
       "  'this',\n",
       "  'life',\n",
       "  'Drink',\n",
       "  'up',\n",
       "  'this',\n",
       "  'love',\n",
       "  ',',\n",
       "  'baby',\n",
       "  ',',\n",
       "  'give',\n",
       "  'it',\n",
       "  'all',\n",
       "  'we',\n",
       "  'got',\n",
       "  'tonight',\n",
       "  'Summer',\n",
       "  'honeysuckle',\n",
       "  'Leaking',\n",
       "  'through',\n",
       "  'a',\n",
       "  'rolled',\n",
       "  'down',\n",
       "  'window',\n",
       "  'We',\n",
       "  'both',\n",
       "  'know',\n",
       "  'when',\n",
       "  'that',\n",
       "  'seat',\n",
       "  'lays',\n",
       "  'back',\n",
       "  'Anything',\n",
       "  'can',\n",
       "  'happen',\n",
       "  'So',\n",
       "  'imagine',\n",
       "  'it',\n",
       "  \"'ll\",\n",
       "  'never',\n",
       "  'end',\n",
       "  'Just',\n",
       "  'close',\n",
       "  'your',\n",
       "  'eyes',\n",
       "  'and',\n",
       "  'you',\n",
       "  'can',\n",
       "  'see',\n",
       "  'that',\n",
       "  'we',\n",
       "  'are',\n",
       "  'where',\n",
       "  'we',\n",
       "  \"'re\",\n",
       "  'meant',\n",
       "  'to',\n",
       "  'be',\n",
       "  'Baby',\n",
       "  'fall',\n",
       "  'into',\n",
       "  'my',\n",
       "  'kiss',\n",
       "  'It',\n",
       "  'should',\n",
       "  'just',\n",
       "  'happen',\n",
       "  'like',\n",
       "  'this',\n",
       "  'Trust',\n",
       "  'it',\n",
       "  'so',\n",
       "  'much',\n",
       "  'that',\n",
       "  'there',\n",
       "  \"'s\",\n",
       "  'no',\n",
       "  'one',\n",
       "  'else',\n",
       "  'but',\n",
       "  'us',\n",
       "  'and',\n",
       "  'This',\n",
       "  'moment',\n",
       "  'that',\n",
       "  'says',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'so',\n",
       "  'right',\n",
       "  \"'Cause\",\n",
       "  'that',\n",
       "  \"'s\",\n",
       "  'all',\n",
       "  'we',\n",
       "  'have',\n",
       "  'in',\n",
       "  'this',\n",
       "  'life',\n",
       "  'Baby',\n",
       "  ',',\n",
       "  'drink',\n",
       "  'up',\n",
       "  'this',\n",
       "  'love',\n",
       "  ',',\n",
       "  'give',\n",
       "  'it',\n",
       "  'all',\n",
       "  'we',\n",
       "  'got',\n",
       "  'tonight',\n",
       "  'Give',\n",
       "  'it',\n",
       "  'all',\n",
       "  'we',\n",
       "  'got',\n",
       "  'tonight',\n",
       "  'Baby',\n",
       "  'fall',\n",
       "  'into',\n",
       "  'my',\n",
       "  'kiss',\n",
       "  'It',\n",
       "  'should',\n",
       "  'just',\n",
       "  'happen',\n",
       "  'like',\n",
       "  'this',\n",
       "  'Trust',\n",
       "  'it',\n",
       "  'so',\n",
       "  'much',\n",
       "  'that',\n",
       "  'there',\n",
       "  \"'s\",\n",
       "  'no',\n",
       "  'one',\n",
       "  'else',\n",
       "  'but',\n",
       "  'us',\n",
       "  'and',\n",
       "  'This',\n",
       "  'moment',\n",
       "  'that',\n",
       "  'says',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'so',\n",
       "  'right',\n",
       "  'That',\n",
       "  \"'s\",\n",
       "  'all',\n",
       "  'we',\n",
       "  'have',\n",
       "  'in',\n",
       "  'this',\n",
       "  'life',\n",
       "  'Drink',\n",
       "  'up',\n",
       "  'this',\n",
       "  'love',\n",
       "  ',',\n",
       "  \"c'mon\",\n",
       "  ',',\n",
       "  'give',\n",
       "  'it',\n",
       "  'all',\n",
       "  'we',\n",
       "  'got',\n",
       "  'tonight',\n",
       "  'Give',\n",
       "  'it',\n",
       "  'all',\n",
       "  'we',\n",
       "  'got',\n",
       "  'tonight',\n",
       "  'SENTENCE_END']]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholeSong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"July moonlight shines Your pretty little head on my shoulder Pull over on the side of the road Oh my God, you're something Like nothing I've ever seen If I'm asleep girl, let me dream  Baby fall into my kiss It should just happen like this Trust it so much that there's no one else but us and This moment that says it's so right 'Cause that's all we have in this life Drink up this love, baby, give it all we got tonight  Summer honeysuckle Leaking through a rolled down window We both know when that seat lays back Anything can happen So imagine it'll never end Just close your eyes and you can see that we are where we're meant to be  Baby fall into my kiss It should just happen like this Trust it so much that there's no one else but us and This moment that says it's so right 'Cause that's all we have in this life Baby, drink up this love, give it all we got tonight Give it all we got tonight  Baby fall into my kiss It should just happen like this Trust it so much that there's no one else but us and This moment that says it's so right That's all we have in this life Drink up this love, c'mon, give it all we got tonight Give it all we got tonight\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLyric.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK', 'UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " [u'a'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK', 'UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', u'a', 'UNK', 'UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " [u'a', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'],\n",
       " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholeSong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_sentence(s, index_to_word):\n",
    "    sentence_str = [index_to_word[x] for x in s[1:-1]]\n",
    "    print(\" \".join(sentence_str))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def generate_sentence(model, index_to_word, word_to_index, min_length=5):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[SENTENCE_START_TOKEN]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[SENTENCE_END_TOKEN]:\n",
    "        next_word_probs = model.predict(new_sentence)[-1]\n",
    "        samples = np.random.multinomial(1, next_word_probs)\n",
    "        sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        # Seomtimes we get stuck if the sentence becomes too long, e.g. \"........\" :(\n",
    "        # And: We don't want sentences with UNKNOWN_TOKEN's\n",
    "        if len(new_sentence) > 100 or sampled_word == word_to_index[UNKNOWN_TOKEN]:\n",
    "            return None\n",
    "    if len(new_sentence) < min_length:\n",
    "        return None\n",
    "    return new_sentence\n",
    "\n",
    "def generate_sentences(model, n, index_to_word, word_to_index):\n",
    "    for i in range(n):\n",
    "        sent = None\n",
    "        while not sent:\n",
    "            sent = generate_sentence(model, index_to_word, word_to_index)\n",
    "        print_sentence(sent, index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "says head Just in in this life life Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "SENTENCE_START Pull be know\n",
      "Baby up tonight tonight c'mon it all we got tonight Give it all we got tonight\n",
      "on I in in this life Drink Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "Just got have have in this life Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "into and it all we got tonight\n",
      "can Oh on fall\n",
      "That Give all all we got tonight Give it all we got tonight\n",
      "We I through Give it it all we got tonight\n",
      "seat have got this life Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "let you we got tonight\n",
      "but we in in this life life Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "rolled your dream in in this life Drink Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "This ever end Just have in this life life Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "be over baby\n",
      "says moonlight got God it all we got tonight\n",
      "know be dream have in this this life Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "little there have in this life life Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "asleep we got tonight\n",
      "'Cause all got tonight\n",
      "'Cause This up in this life life Drink up this love , c'mon , give it all we got tonight Give it all we got tonight\n",
      "<MASK/> Oh Your\n",
      "got c'mon it it all we got tonight Give it all we got tonight\n",
      "all got tonight\n",
      "ever honeysuckle 'Cause else\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(gru_country2, 25, index_to_word, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'Cause\": 74,\n",
       " \"'ll\": 65,\n",
       " \"'m\": 64,\n",
       " \"'re\": 73,\n",
       " \"'s\": 110,\n",
       " \"'ve\": 63,\n",
       " ',': 108,\n",
       " '<MASK/>': 0,\n",
       " 'Anything': 62,\n",
       " 'Baby': 103,\n",
       " 'Drink': 72,\n",
       " 'Give': 71,\n",
       " 'God': 61,\n",
       " 'I': 70,\n",
       " 'If': 60,\n",
       " 'It': 99,\n",
       " 'Just': 59,\n",
       " 'Leaking': 58,\n",
       " 'Like': 57,\n",
       " 'Oh': 56,\n",
       " 'Pull': 55,\n",
       " 'SENTENCE_STARTJuly': 54,\n",
       " 'So': 53,\n",
       " 'Summer': 52,\n",
       " 'That': 51,\n",
       " 'This': 98,\n",
       " 'Trust': 97,\n",
       " 'UNKNOWN_TOKEN': 1,\n",
       " 'We': 50,\n",
       " 'Your': 49,\n",
       " 'a': 48,\n",
       " 'all': 107,\n",
       " 'and': 102,\n",
       " 'are': 47,\n",
       " 'asleep': 46,\n",
       " 'baby': 45,\n",
       " 'back': 44,\n",
       " 'be': 43,\n",
       " 'both': 42,\n",
       " 'but': 96,\n",
       " \"c'mon\": 41,\n",
       " 'can': 69,\n",
       " 'close': 40,\n",
       " 'down': 39,\n",
       " 'dream': 38,\n",
       " 'drink': 37,\n",
       " 'else': 95,\n",
       " 'end': 36,\n",
       " 'ever': 35,\n",
       " 'eyes': 34,\n",
       " 'fall': 94,\n",
       " 'girl': 33,\n",
       " 'give': 93,\n",
       " 'got': 105,\n",
       " 'happen': 101,\n",
       " 'have': 92,\n",
       " 'head': 32,\n",
       " 'honeysuckle': 31,\n",
       " 'imagine': 30,\n",
       " 'in': 91,\n",
       " 'into': 90,\n",
       " 'it': 113,\n",
       " 'just': 89,\n",
       " 'kiss': 88,\n",
       " 'know': 29,\n",
       " 'lays': 28,\n",
       " 'let': 27,\n",
       " 'life': 87,\n",
       " 'like': 86,\n",
       " 'little': 26,\n",
       " 'love': 85,\n",
       " 'me': 25,\n",
       " 'meant': 24,\n",
       " 'moment': 84,\n",
       " 'moonlight': 23,\n",
       " 'much': 83,\n",
       " 'my': 104,\n",
       " 'never': 22,\n",
       " 'no': 82,\n",
       " 'nothing': 21,\n",
       " 'of': 20,\n",
       " 'on': 68,\n",
       " 'one': 81,\n",
       " 'over': 19,\n",
       " 'pretty': 18,\n",
       " 'right': 80,\n",
       " 'road': 17,\n",
       " 'rolled': 16,\n",
       " 'says': 79,\n",
       " 'seat': 15,\n",
       " 'see': 14,\n",
       " 'seen': 13,\n",
       " 'shines': 12,\n",
       " 'should': 78,\n",
       " 'shoulder': 11,\n",
       " 'side': 10,\n",
       " 'so': 106,\n",
       " 'something': 9,\n",
       " 'that': 112,\n",
       " 'the': 67,\n",
       " 'there': 77,\n",
       " 'this': 109,\n",
       " 'through': 8,\n",
       " 'to': 7,\n",
       " 'tonight': 100,\n",
       " 'tonightSENTENCE_END': 6,\n",
       " 'up': 76,\n",
       " 'us': 75,\n",
       " 'we': 111,\n",
       " 'when': 5,\n",
       " 'where': 4,\n",
       " 'window': 3,\n",
       " 'you': 66,\n",
       " 'your': 2}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"July moonlight shines Your pretty little head on my shoulder Pull over on the side of the road Oh my God, you're something Like nothing I've ever seen If I'm asleep girl, let me dream  Baby fall into my kiss It should just happen like this Trust it so much that there's no one else but us and This moment that says it's so right 'Cause that's all we have in this life Drink up this love, baby, give it all we got tonight  Summer honeysuckle Leaking through a rolled down window We both know when that seat lays back Anything can happen So imagine it'll never end Just close your eyes and you can see that we are where we're meant to be  Baby fall into my kiss It should just happen like this Trust it so much that there's no one else but us and This moment that says it's so right 'Cause that's all we have in this life Baby, drink up this love, give it all we got tonight Give it all we got tonight  Baby fall into my kiss It should just happen like this Trust it so much that there's no one else but us and This moment that says it's so right That's all we have in this life Drink up this love, c'mon, give it all we got tonight Give it all we got tonight\""
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedLyrics[1][2].replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"/Users/carterraha/Desktop/artists/etc/lyricshere2.csv\",'w') as f:\n",
    "    writer = csv.writer(f,delimiter=\"@\")\n",
    "    for lyric in processedLyrics:\n",
    "        writer.writerow([lyric[2].replace(\"\\n\", \" \").strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import operator\n",
    "from utils import load_data, load_model_parameters_theano, generate_sentences, train_with_sgd\n",
    "from gru_theano import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 16292 sentences.\n",
      "Found 16375 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'tahoe' and appeared 2 times.\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_SIZE = 8000\n",
    "X_train, y_train, word_to_index, index_to_word = load_data(\"/Users/carterraha/Desktop/artists/etc/lyricshere2.csv\", VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gru_theano.py:112: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  [x, y, learning_rate, theano.Param(decay, default=0.9)],\n"
     ]
    }
   ],
   "source": [
    "# Build your own model (not recommended unless you have a lot of time!)\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "NEPOCH = 20\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "model = GRUTheano(VOCABULARY_SIZE, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Step time: ~586.803913 milliseconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "model.sgd_step(X_train[0], y_train[0], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print \"SGD Step time: ~%f milliseconds\" % ((t2 - t1) * 1000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Step time: ~19985829.493046 milliseconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "train_with_sgd(model, X_train, y_train, LEARNING_RATE, 1, decay=0.9)\n",
    "t2 = time.time()\n",
    "print \"SGD Step time: ~%f milliseconds\" % ((t2 - t1) * 1000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if every this wrong it let it 's like a me\n",
      "got me there i always\n",
      "she everywhere living you room and the say she 's girl\n",
      "about the big out on the summer day us home there at the the a i would do would things that i 'd go in down the road her took off the and he was so but i ai n't ever been for ! ''\n",
      "keep that 's a realizing one life to a his from a n't got in time .\n",
      "i 'm that my , it man that 's way .\n",
      "but i do n't know her some and .\n",
      "when always 'd winner when we say you is the you there out ( yeah ) , can we wan na go here , us for a palmetto heart love is drive ?\n",
      "what she left in her before she , when i was and you where it sometimes you were i was to you at home , little can i life that it s only but i her behind you i do through .\n",
      "i can down at you ’ lonely back , i day you say `` you ca n't gon na look boots that '' like you over now , well that tried to make it baby to love we here for you and you know how to got bar\n",
      "it i my life i think i out of this mine .\n",
      "you on my way that night i do n't know much to unfold you love good hear been yawnin ' on my back you 've got find a little and you 'll right found in it that 's did you 've been know that love just do n't but i no i 've been in those life and 'll said\n",
      "i said you ever fearless me away and she could n't see you goodbye late ever do you if i just a little farewell for the as can get and tow out this\n",
      "it ’ s a of good only girl new in down a win , that ’ s a `` girl '' got love .\n",
      "be and that 's all let in now .\n",
      "( all the hard ? )\n",
      "`` me like it all of that and all asked but i they way from could you girl with your pourin ' be get out of heart it ?\n",
      "it 's ever just got on your .\n",
      "is on it .\n",
      "as i love you my .\n",
      "my man do n't my but we could you make a a better then .\n",
      "`` streets survived to girl no one 'd how . ''\n",
      "`` me and when it 's keg to big .\n",
      "i ' run never not let if you still rain for it\n",
      "they play with fight toughest , your lowry stay a an time now go to ride on our fast sun .\n",
      "hold me eyes .\n",
      "`` will you face and the they gon na old never get you home , of you ?\n",
      "( pushes to hold 'll a what in had that ) up\n",
      "ca n't all boy do n't tree i i feel you a world and it i thing in out there through the no really how to was and sure heart hold you last you be my back never get feel to me you 're\n",
      "[ lot all my\n",
      "there 's a with the well cause your mind what every when i how it 's just a he answer my overwhelms just an again much again i we go `` but always wo in when you look baby\n",
      "wrong much not to see our mm\n",
      "she if you could old .\n",
      "what you hell , be .\n",
      "end it leavin ' you 're really guess we 're all want to a\n",
      "sky ’ of the you is my back to moonlit\n",
      "i ' something things long to that much like me but you never shot me\n",
      "that i it is there\n",
      "there 's a or a dragouts can miss god right she crazy her n't old love .\n",
      "if the 've wider to this out on in that .\n",
      "i got a always got on place in baby not you i 'm miss you next time to want to the time girl that i would your home but we your ?\n",
      "as the see that light hand .\n",
      "back with the backyard when home my way words and me\n",
      "it teased on a bestest radio it 's should old man on the when fire world there 's a but ai n't ?\n",
      "they know away from control out i 'm so much i take my world my wait your room and know it know you 's stroke of can we feel the way we got it was all a gene my mind and i i 'd hold so to there not you too me to of but hard to daddy\n",
      "i believe in love .\n",
      "there get or one , what new life you can i after all be to failures , but it 's her cuddled , bubbles and a charge of my love it just got ( on he\n",
      "the got within all my boat song , and the should they that you tired the long down telling old how up on you as the melt came on a were all is do you ai n't at all there in the was all from the door as a midnight for his wan na to myself that i 'd just is on the town .\n",
      "long say it is a gale in it little the me back home and let our wo you do the your house all a was so we can your on the we about take a shelton i baby why say i tell nothing were be hands\n",
      "i took her about my little ?\n",
      "but you me and i little alone i got i love you the life love for so she barstool you me on every way i now i out in the way it was were a ohh .\n",
      "the that humble on your heart the tours of your still song , you i have my up when i ' oh , i had it , your icy make me go , old my home and i got my am it was a 's make tellin ' a child , i before nd i ' 'd never people so why good for the good after all the what i so left to that moon dying , and there 's no way .\n",
      "she it 's a when i as it a did the whole\n",
      "all in the lights get tell good to hell like just this had but about this bar i been got a hard so gon na be but than a\n",
      "made wan she .\n",
      "they say two 've got then says i 'm in my road how true we 'm back to of ?\n",
      "every morning in the keg\n",
      "away love a my from where i love the the good came on like his things they me out and tell it were all but my heart was n't gone tonight you say you came a town cause you love for me but i of my away am the where no\n",
      "maybe i 've been some let here 's gon of oh come in when i there 's even do the life i be i see seen the the that trying might come to the your by over oh i end up\n",
      "'s take the alabama ) how i i keep going in love could while my , there that for some ' me and i town ago give let me from to race , 'm around in that little 's off the everybody were for away so\n",
      "verse know .\n",
      "know it for her heart could know .\n",
      "i i 'm sittin ' in a can thing if i in mind but i girl\n",
      "i feeling see\n",
      "or the fire ai n't town but it 's , tell me what we high .\n",
      "it were n't home\n",
      "they 're never have to at and tell you here i will .\n",
      "`` what i home where they came ? ''\n",
      "me on the compass right yeah the had to ever but i know that i my heart but there was one for the world yeah me when you make my `` , the do n't when did n't no one run in a are he got why me old down in the movie i a what i from i tried to sweet like before old song that was 'm so just always to never gone he 's waiting for the nose `` could for ask in my this ?\n",
      "i never knew it was too old .\n",
      "this old my well i i do n't like no\n",
      "24 mack 're lullaby what is ? ''\n",
      "it 's so ; i 'm there ai n't end\n",
      "day never been sweat\n",
      "up in right now i love every home i cry to girl i say it 's friend always 'll last to long me to the out of radio up off my i 'm in were jamaican that 's man i 'm just a in and i 'm not some buzzed never 're just that night and for my his old heart is n't for two and old tomb 's his and when i 'm but i know if time would 's there `` all i about ?\n",
      "so france in ) well that was the writin '\n",
      "and the old on the , stay much 'm tell their world on the some the run no ever drive away but now walked out over you\n",
      "if if i 'm a you i just ca n't every here gon na not night you out of a away going from a blamed there 's a but me time think i 'm just everything i feel you stop it here go home i there got it something , wan na better ( you place ) it ai n't for the draggin ' , i mind a\n",
      "there 's there\n",
      "’ ’ way up town .\n",
      "who 's peace in she a by\n",
      "them he are home i shattered left from you at home oh song more and go there baby\n",
      "( what there they can a .\n",
      "one you , old 'm from that`s , i town no hold on me while on would do n't so go as good as i feel the around time something should them first up road\n",
      "if it 's so many these many it '' eyes with me\n",
      "right now the i take an go back up to you take but i chorus ta pick up\n",
      "his all the love every birds and the drummers 's hear life the ( my man the mind till that be one you her off he said the answering and he the 's long somewhere he old whipperwill touch the and his you was n't say n't love now but if i on the the as i 'm heels and i know care .\n",
      "her bad stuff\n",
      "she then with me down on the my right they were but ai n't no\n",
      "you say who 's so far away ?\n",
      "he got a we like us comes down on the your see one nothing 's those it off your far i for do n't it world left to did n't without love for for it\n",
      "( yeah )\n",
      "then you the too a of the home .\n",
      "like all and if i home n't by the door when i leave love as at home i how since but i i not my you oh , my through when you 're on me had you coming home .\n",
      "i ca n't be how i something in the love i give but my heart 's lost last yeah sun love 's just tell them you let me go had time to that a get little boy to go time and it is so how lot it up by all your got back than your better he it was just 'll that me\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nena out of\n",
      "i house ’ too n't off more than why i no one ’ was i love pouring some road so i ’ now i if you got me a but 've got a about all you was two and it ’ s n't all to but i ’ m you ’ i ’ ll play my hands in the what you have it ’ s , so i will out here like that who so much you good ’ like the way you girl i in her eyes and no\n",
      "might his `` and your you\n",
      "here go over on the you why do i the that to not me and my everything there is you ’ boy in an when i ’ m ' never need for you again ’ s stick up over want to you have how everybody two stars up wanted made the stopped my was the me i 've keep from all down my `` will be the light ''\n",
      "you 're kids her like now she 's not sure me when the way you i said around by do n't i been 'm get me was resistance just i to as the high corvettes 'll you i me that we 'm that you ca n't just be you your on your way you can find the 'd true and got with time who 's your know got your love it 's high 'll dark all ?\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(model, 100, index_to_word, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model_parameters_theano(model, outfile):\n",
    "    np.savez(outfile,\n",
    "        E=model.E.get_value(),\n",
    "        U=model.U.get_value(),\n",
    "        W=model.W.get_value(),\n",
    "        V=model.V.get_value(),\n",
    "        b=model.b.get_value(),\n",
    "        c=model.c.get_value())\n",
    "    print \"Saved model parameters to %s.\" % outfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model parameters to /Users/carterraha/Desktop/artists/model/params6.npz.\n"
     ]
    }
   ],
   "source": [
    "save_model_parameters_theano(model, \"/Users/carterraha/Desktop/artists/model/params6.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model_parameters_theano('./data/pretrained.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Epochs completed \n",
    "# 1,2 - Oct 18 overnight \n",
    "# 3 - Oct 19 AM - took almost the whole day (roughly 10 to 4:30)\n",
    "# 4 - Oct 19 PM\n",
    "# 5 - Oct 20 Day\n",
    "# 6 - Oct 20 overnight\n",
    "# 7 - Oct 21 overnight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
